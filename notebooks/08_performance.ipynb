{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Performance tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Topics\n",
    "- Why Julia is fast\n",
    "- LLVM compiler\n",
    "- Parallellization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why is Julia fast?\n",
    "- Rich type information, provided naturally by multiple dispatch\n",
    "- Aggressive code specialization against run-time types\n",
    "- JIT compilation using the LLVM compiler framework\n",
    "\n",
    "In short, Julia is designated from the beginning to be fast. Not vice versa.\n",
    "\n",
    "See the [scientific paper](https://arxiv.org/pdf/1209.5145v1.pdf) behind Julia, if you want to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing nested loops: index \"convention\"\n",
    "\n",
    "Julia is a column-major programming language. The inner loop should concern rows rather than columns. This is due to how arrays are stored in memory.\n",
    "\n",
    "What this means is: **first index changes fastest**.\n",
    "```julia\n",
    "for k in 1:Nz\n",
    "    for j in 1:Ny\n",
    "        for i in 1:Nx\n",
    "            arr[i,j,k]\n",
    "        end\n",
    "    end\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is badly written, because it looks at every column of a row, then at \n",
    "# every column of the next row, and so on\n",
    "\n",
    "function laplacian_bad(lap_x::Array{Float64,2}, x::Array{Float64,2})\n",
    "    nr,nc = size(x)\n",
    "    for ir = 2:nr-1 \n",
    "        for ic = 2:nc-1 # bad loop nesting order\n",
    "            lap_x[ir,ic] =\n",
    "                (x[ir+1,ic] + x[ir-1,ic] +\n",
    "                x[ir,ic+1] + x[ir,ic-1]) - 4*x[ir,ic]\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this version, the two loops are nested properly:\n",
    "\n",
    "function laplacian_good(lap_x::Array{Float64,2}, x::Array{Float64,2})\n",
    "    nr,nc = size(x)\n",
    "    for ic = 2:nc-1\n",
    "        for ir = 2:nr-1 # good loop nesting order\n",
    "            lap_x[ir,ic] =\n",
    "                (x[ir+1,ic] + x[ir-1,ic] +\n",
    "                x[ir,ic+1] + x[ir,ic-1]) - 4*x[ir,ic]\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see the effect in practise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Printf\n",
    "function main_test(nr, nc)\n",
    "    field = zeros(nr, nc)\n",
    "    for ic = 1:nc, ir = 1:nr\n",
    "        if ir == 1 || ic == 1 || ir == nr || ic == nc\n",
    "            field[ir,ic] = 1.0\n",
    "        end\n",
    "    end\n",
    "    lap_field = zeros(size(field))\n",
    "\n",
    "    time = @elapsed laplacian_bad(lap_field, field)\n",
    "    @printf \"laplacian_bad:          %.3f s\\n\" time\n",
    "    \n",
    "    time = @elapsed laplacian_good(lap_field, field)\n",
    "    @printf \"laplacian_good:         %.3f s\\n\" time\n",
    "\n",
    "end\n",
    "main_test(10^4, 10^4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Levels of parallellism\n",
    "1. Instruction level parallellism\n",
    "2. Vector instructions (see `Bonus_simd-vectorization.pynb` if you are interested)\n",
    "3. **Threading** (shared-memory)\n",
    "4. **Distributed**\n",
    "5. Accelerators (e.g., GPGPU; *not covered here*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advanced: A (very short) introduction to the interiors of Julia compiler\n",
    "\n",
    "Let's stop treating our tools like blackboxes. Let's see what the compiler herself is thinking about our code with `using InteractiveUtils`.\n",
    "\n",
    "1. `@code_lowered`\n",
    "2. `@code_typed` and `@code_warntype`\n",
    "3. `@code_llvm`\n",
    "4. `@code_native`\n",
    "\n",
    "See [slides](https://slides.com/valentinchuravy/julia-parallelism) by Valentin Churavy, for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "function mysum(A)\n",
    "    acc = zero(eltype(A))\n",
    "    for a in A\n",
    "        acc += a\n",
    "    end\n",
    "    return acc\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "using InteractiveUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "@code_lowered mysum(ones(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "@code_typed mysum(ones(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "@code_llvm mysum(ones(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "@code_native mysum(ones(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About global scope\n",
    "So what does the previous machine code actually mean? Well, in practise:\n",
    "\n",
    "A global variable might have its value, and therefore its type, change at any given point. This makes it difficult/nigh impossible for the compiler to reason about/optimize code using global variables.\n",
    "\n",
    "Julia uses functions as its compilation unit and any code that is performance critical or being benchmarked should be inside a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demo: Giving hints to the compiler \n",
    "Let's see what we can do with our previously defined `laplacian` function. \n",
    "\n",
    "For some aggressive cases we can provide the JIT-compiler `@inbounds` macro hinting that there is no need to check array bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Lets re-define our previous Laplacians (from 05_ notebook)\n",
    "function laplacian_bad(lap_x::Array{Float64,2}, x::Array{Float64,2})\n",
    "    nr,nc = size(x)\n",
    "    for ir = 2:nr-1, ic = 2:nc-1 # bad loop nesting order\n",
    "        lap_x[ir,ic] =\n",
    "            (x[ir+1,ic] + x[ir-1,ic] +\n",
    "            x[ir,ic+1] + x[ir,ic-1]) - 4*x[ir,ic]\n",
    "    end\n",
    "end\n",
    "\n",
    "#In this version, the two loops are nested properly:\n",
    "function laplacian_good(lap_x::Array{Float64,2}, x::Array{Float64,2})\n",
    "    nr,nc = size(x)\n",
    "    for ic = 2:nc-1, ir = 2:nr-1 # good loop nesting order\n",
    "        lap_x[ir,ic] =\n",
    "            (x[ir+1,ic] + x[ir-1,ic] +\n",
    "            x[ir,ic+1] + x[ir,ic-1]) - 4*x[ir,ic]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# A way to increase the speed is to remove the array bounds checking, using the macro @inbounds:\n",
    "function laplacian_good_nocheck(lap_x::Array{Float64,2}, x::Array{Float64,2})\n",
    "    nr,nc = size(x)\n",
    "    for ic = 2:nc-1\n",
    "        for ir = 2:nr-1 # good loop nesting order\n",
    "            @inbounds begin lap_x[ir,ic] = # no array bounds checking\n",
    "                (x[ir+1,ic] +  x[ir-1,ic] +\n",
    "                x[ir,ic+1] + x[ir,ic-1]) - 4*x[ir,ic]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "using Printf #evaluate me to get printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "function main_test(nr, nc)\n",
    "    field = zeros(nr, nc)\n",
    "    for ic = 1:nc, ir = 1:nr\n",
    "        if ir == 1 || ic == 1 || ir == nr || ic == nc\n",
    "            field[ir,ic] = 1.0\n",
    "        end\n",
    "    end\n",
    "    lap_field = zeros(size(field))\n",
    "\n",
    "    time = @elapsed laplacian_bad(lap_field, field)\n",
    "    @printf \"laplacian_bad:          %.3f s\\n\" time\n",
    "    \n",
    "    time = @elapsed laplacian_good(lap_field, field)\n",
    "    @printf \"laplacian_good:         %.3f s\\n\" time\n",
    "    \n",
    "    time = @elapsed laplacian_good_nocheck(lap_field, field)\n",
    "    @printf \"laplacian_good_nocheck: %.3f s\\n\" time\n",
    "end\n",
    "\n",
    "main_test(10^4, 10^4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Threading (experimental)\n",
    "Julia threading model is based on a fork-join approach and is still considered experimental.\n",
    "\n",
    "Fork-join describes the control flow that a group of threads undergoes. Execution is then forked and an anonymous function is ran across all threads.\n",
    "\n",
    "All threads have to join together and serial execution continues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Threading in practise\n",
    "The number of threads Julia starts up with is controlled by an environment variable called `JULIA_NUM_THREADS`. Now, let's start up Julia with 4 threads:\n",
    "\n",
    "```bash\n",
    "export JULIA_NUM_THREADS=4\n",
    "julia\n",
    "```\n",
    "\n",
    "NOTE: this does not work in the notebook environment because the kernel is automatically loaded with only 1 thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "using Base.Threads\n",
    "nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using threads\n",
    "\n",
    "```julia\n",
    "@threads for id in 1:nthreads()\n",
    "    #each thread does something\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a = zeros(10)\n",
    "@threads for i = 1:10\n",
    "    a[i] = Threads.threadid()\n",
    "end\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Advanced: Threaded sum\n",
    "Here is a more complex example of threaded sum from https://github.com/stevengj/18S096/blob/master/lectures/lecture5/Parallelism.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "function threaded_sum(arr)\n",
    "   @assert length(arr) % nthreads() == 0\n",
    "    \n",
    "   let results = zeros(eltype(arr), nthreads())\n",
    "       @threads for tid in 1:nthreads()\n",
    "           # split work\n",
    "           acc = zero(eltype(arr))\n",
    "           len = div(length(arr), nthreads())\n",
    "           domain = ((tid-1)*len +1):tid*len\n",
    "           @inbounds for i in domain\n",
    "               acc += arr[i]    \n",
    "           end\n",
    "           results[tid] = acc\n",
    "       end\n",
    "       sum(results)\n",
    "   end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distributed computing \n",
    "Distributed processing uses individual processes that communicate with each other. In this case, data movement and communication is explicit!\n",
    "\n",
    "Julia supports various forms of distributed computing. \n",
    "- **A native master-worker system based on remote procedure calls**\n",
    "- MPI through [MPI.jl](https://github.com/JuliaParallel/MPI.jl)\n",
    "- [DistributedArrays.jl](https://github.com/JuliaParallel/DistributedArrays.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Master-Worker model\n",
    "We need to launch Julia with \n",
    "```bash\n",
    "julia -p 4\n",
    "```\n",
    "then inside Julia you can check\n",
    "```julia\n",
    "nprocs()\n",
    "workers()\n",
    "```\n",
    "which should print `5` and `[2,3,4,5]`. \n",
    "\n",
    "Why 5, you ask? Because *\"worker 1\"* is the *\"boss\"*. And bosses don't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Functions (and everything used by workers) needs to be explicitly declared for all:\n",
    "```julia\n",
    "@everywhere g(x) = 2x\n",
    "```\n",
    "Only then can we send the job to somebody else and fetch the result\n",
    "```julia\n",
    "remotecall_fetch(g, 3, 2.0)\n",
    "```\n",
    "Here we fetch the result of `g` of worker `3` applied to a value of `2.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use `@everywhere` to execute a top-level block on each process\n",
    "```julia\n",
    "@everywhere begin\n",
    "    using Test\n",
    "    include(\"src.jl\")\n",
    "end\n",
    "```\n",
    "Define variables on all processes\n",
    "```julia\n",
    "@everywhere bar = 1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `@distributed` as a shortcut \n",
    "A parallel for loop of the form (from `using Distributed`):\n",
    "```julia\n",
    "@distributed [reducer] for var = range\n",
    "    body\n",
    "end\n",
    "```\n",
    "The specified range is partitioned and locally executed across all workers. In case an optional reducer function is specified, `@distributed` performs local reductions on each worker with a final reduction on the calling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that without a reducer function, `@distributed` executes asynchronously, i.e., it spawns independent tasks on all available workers and returns immediately without waiting for completion. To wait for completion, prefix the call with `@sync`, like :\n",
    "```julia\n",
    "@sync @distributed for var = range\n",
    "      body\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using Distributed\n",
    "\n",
    "nheads = @distributed (+) for i=1:200000000\n",
    "  Int(rand(Bool))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## pmap for unbalanced load\n",
    "In some cases no reduction operator is needed, and we merely wish to apply a function to all integers in some range. This is another useful operation called parallel map. \n",
    "\n",
    "For example, we could compute the singular values of several large random matrices in parallel as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra #loading svd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "M = Matrix{Float64}[rand(1000,1000) for i=1:10]\n",
    "pmap(svd, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`pmap()` is designed for the case where each function call does a large amount of work. In contrast, `@distributed for` can handle situations where each iteration is tiny, perhaps merely summing two numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: π in parallel\n",
    "\n",
    "Let's compute the value of π=3.1415926.... in parallel using Monte Carlo simulations.\n",
    "\n",
    "Below is a function that does this by throwing darts inside a box of $[-1, 1]^2$. It follows from the definition of π that the area fo a circle is $A = \\pi r^2$, where $r$ is the radius of a circle. We develop a Monte Carlo algorithm to compute π by randomly throwing darts (i.e., generating uniformly distributed random numbers inside a unit box) and counting the fraction of the points that land inside the circle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to visualize the setup first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "pyplot()\n",
    "\n",
    "plt = plot( xlims=(-2,2), ylims=(-2,2), aspect_ratio=1)\n",
    "for n in 1:500\n",
    "    x = rand()*2 - 1\n",
    "    y = rand()*2 - 1\n",
    "    \n",
    "    r = sqrt(x^2 + y^2)\n",
    "    if r < 1\n",
    "        #inside\n",
    "        scatter!([x], [y], color=\"red\", label=\"\")\n",
    "    else\n",
    "        #outside\n",
    "        scatter!([x], [y], color=\"blue\",label=\"\")\n",
    "    end\n",
    "end\n",
    "plt #display plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see the circle? Now, let's do some actual computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_pi(N::Int)\n",
    "    n_landed_in_circle = 0  # counts number of points that have radial coordinate < 1, i.e. in circle\n",
    "    for i = 1:N\n",
    "        x = rand() * 2 - 1  # uniformly distributed number on x-axis\n",
    "        y = rand() * 2 - 1  # uniformly distributed number on y-axis\n",
    "\n",
    "        r = sqrt(x*x + y*y)  # radius squared, in radial coordinates\n",
    "        if r < 1\n",
    "            n_landed_in_circle += 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return n_landed_in_circle / N * 4.0    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time macro is a quick way to benchmark the performance of our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time compute_pi(10^9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual exercise\n",
    "Your mission? Parallellize the `compute_pi` function! \n",
    "\n",
    "Hint: see the `for` loop? Remember `@distributed`? Remember that when using `@distributed` the result of each iteration is taken as the value of the last expression inside the loop. Therefore if your loops ends in\n",
    "```julia\n",
    "for \n",
    "    #blaablaa\n",
    "    #...\n",
    "    if something\n",
    "        1\n",
    "    else\n",
    "        0\n",
    "    end\n",
    "end\n",
    "```\n",
    "it will result in returning either `1` or `0`.\n",
    "\n",
    "You can also try adapting this to use `pmap` if you like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to add workers before we run our sweet new function with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed\n",
    "addprocs(4)\n",
    "nprocs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after you are done, see the full story at [Parallel Monte Carlo in Julia](http://corysimon.github.io/articles/parallel-monte-carlo-in-julia/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Exercise 1: Distributed Arrays\n",
    "\n",
    "Install `DistributedArrays` as\n",
    "```julia\n",
    "Pkg.add(\"DistributedArrays\")\n",
    "```\n",
    "\n",
    "then try them out as\n",
    "```julia\n",
    "addprocs(4) #adding 4 workers to share the load\n",
    "@everywhere using DistributedArrays #loading DAs for every worker\n",
    "A = fill(1.1, (100, 100) ) #create array\n",
    "DA = distribute(A) #distribute it to workers\n",
    "sum(DA)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Exercise 2: MPI\n",
    "\n",
    "Install `MPI` package by running:\n",
    "```julia\n",
    "Pkg.update()\n",
    "Pkg.add(\"MPI\")\n",
    "```\n",
    "In case of problems, see the [readme](https://github.com/JuliaParallel/MPI.jl)\n",
    "\n",
    "Because of how MPI works, we need to explicitly write our code into a file. Create `01-hello.jl` and `01-hello-impl.jl` as follows:\n",
    "\n",
    "`01-hello.jl` should look like this:\n",
    "```julia\n",
    "\n",
    "import MPI\n",
    "include(\"01-hello-impl.jl\")\n",
    "\n",
    "function main()\n",
    "    MPI.Init()\n",
    "\n",
    "    do_hello()\n",
    "\n",
    "    MPI.Finalize()\n",
    "end\n",
    "\n",
    "main()\n",
    "```\n",
    "and the actual implementation file `01-hello-impl.jl` like this\n",
    "```julia\n",
    "\n",
    "function do_hello()\n",
    "    comm = MPI.COMM_WORLD\n",
    "    println(\"Hello world, I am $(MPI.Comm_rank(comm)) of $(MPI.Comm_size(comm))\")\n",
    "    MPI.Barrier(comm)\n",
    "end\n",
    "```\n",
    "\n",
    "\n",
    "You can execute your code the normal way as\n",
    "```bash\n",
    "mpirun -np 3 julia 01-hello.jl\n",
    "```\n",
    "\n",
    "See the MPI.jl [examples](https://github.com/JuliaParallel/MPI.jl/tree/master/examples) for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary: General optimization tricks\n",
    "\n",
    "- Write functions!\n",
    "- Avoid global variables\n",
    "    - A global variable might have its value, (and type) change at any given point. This makes it hard for the compiler to optimize."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
